import boto3
import yaml
import logging
import time
from awsglue.context import GlueContext
from pyspark.context import SparkContext

import great_expectations as gx
from great_expectations.checkpoint import SimpleCheckpoint
from great_expectations.core.batch import RuntimeBatchRequest
from great_expectations.data_context import BaseDataContext
from great_expectations.data_context.types.base import (
    DataContextConfig,
    S3StoreBackendDefaults,
)

logger = logging.getLogger(__name__)
FORMAT = "[%(asctime)s] [%(threadName)s] %(levelname)s: %(message)s"
logging.basicConfig(level=logging.INFO, format=FORMAT)

sc = SparkContext.getOrCreate()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

s3_client = boto3.client("s3")
response = s3_client.get_object(
    Bucket="soname-gx-demo-data", Key="ge/great_expectations/great_expectations.yml"
)
config_file = yaml.safe_load(response["Body"])

config = DataContextConfig(
    config_version=config_file["config_version"],
    datasources=config_file["datasources"],
    expectations_store_name=config_file["expectations_store_name"],
    validations_store_name=config_file["validations_store_name"],
    evaluation_parameter_store_name=config_file["evaluation_parameter_store_name"],
    plugins_directory="/great_expectations/plugins",
    stores=config_file["stores"],
    data_docs_sites=config_file["data_docs_sites"],
    config_variables_file_path=config_file["config_variables_file_path"],
    anonymous_usage_statistics=config_file["anonymous_usage_statistics"],
    checkpoint_store_name=config_file["checkpoint_store_name"],
    store_backend_defaults=S3StoreBackendDefaults(
        default_bucket_name=config_file["data_docs_sites"]["s3_site"]["store_backend"][
            "bucket"
        ]
    ),
)
context_gx = BaseDataContext(project_config=config)
logger.info("initiated ge context")
expectation_suite_name = "suite_gdw_int_veh"
my_checkpoint_name = "checkpoiont_int_veh"



yaml_config = f"""
---
name: {my_checkpoint_name}
config_version: 1.0
template_name:
module_name: great_expectations.checkpoint
class_name: Checkpoint
run_name_template: '%Y-%m-%d-int-veh'
expectation_suite_name:
batch_request: {{}}
action_list:
  - name: store_validation_result
    action:
      class_name: StoreValidationResultAction
  - name: store_evaluation_params
    action:
      class_name: StoreEvaluationParametersAction
  - name: update_data_docs
    action:
      class_name: UpdateDataDocsAction
evaluation_parameters: {{}}
runtime_configuration:
validations:
  - name: int-veh check
    batch_request:
      datasource_name: spark_s3
      data_connector_name: default_runtime_data_connector_name
      data_asset_name: int_veh.csv
      batch_identifiers:
        runtime_batch_identifier_name: default_identifier
      runtime_parameters: 
        path: s3://soname-gx-demo-data/data/int_veh.csv
      batch_spec_passthrough:
        reader_options:
          sep: '#'
          header: true
    expectation_suite_name: {expectation_suite_name}
profilers: []
ge_cloud_id:
expectation_suite_ge_cloud_id:
"""

my_checkpoint = context_gx.test_yaml_config(yaml_config=yaml_config)
context_gx.add_checkpoint(**yaml.load(yaml_config))
start = time.perf_counter()
result = context_gx.run_checkpoint(checkpoint_name=my_checkpoint_name)
end = time.perf_counter()
logger.warning(f"Result success is {result['success']}. Calculation took {((end - start)/60):.2f} seconds")
context_gx.build_data_docs()
